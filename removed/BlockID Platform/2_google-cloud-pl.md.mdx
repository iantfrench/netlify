---
slug: /v1/google-cloud-platform-kubernetes-architecture-guide-for-blockid-platform
id: Zf3f90934-5f97-46a4-a2bf-8b101c4aee00
---
# Google Cloud Platform: Kubernetes architecture guide for BlockID Platform
The BlockID platform introduces multi-tenancy in the application which means the application will logically have multiple tenants for each tenant of the customer. It also includes a single tenancy model for those customers who want to act as the only tenant within the application. This document explains the BlockID platform architecture in Kubernetes (K8) on the Google Cloud Platform (GCP).

![BlockID Platform - Multi-Single-Tenancy.png](https://cdn.document360.io/aeae7c88-d2f4-4a9c-bba9-c4f368260d78/Images/Documentation/BlockID%20Platform%20-%20Multi-Single-Tenancy.png)

The BlockID platform architecture has the following two logically separate blocks:

1. Blocks that reside within Kubernetes on GCP
2. Blocks that reside outside of GCP, for example, other managed services such as MongoDB, mobile applications such as BlockID.

## Outside of the BlockID Platform structure
The following elements are connected from outside to the BlockID platform structure:
1. **BlockID mobile application**: This application allows your users to log in to their accounts leveraging their biometrics. The biometric options include TouchID / FaceID, and LiveID.
2. **Two BlockID platform tenants**:
    * **With BlockID Authentication Broker**: This tenant creates an outbound connection to a BlockID platform structure for the customer and connects back to the active directory (AD) of the customer.
    * **Without BlockID Authentication Broker**: Few tenants might not need the broker to connect to their AD in case of the following conditions:
         * The tenant has a separate LDAP or AD instance that is directly reachable to the BlockID console.
        * The separate endpoint of the LDAP or AD instance in the AWS 1KUD within the BlockID platform is managing the LDAP service for them.
        * The separate endpoint of the LDAP or AD instance in the MongoDB Atlas within the BlockID platform is managing the LDAP service for them.
3. **AWS 1KUD LDAP**: 1KUD is for the customers who do not own the LDAP or AD server (or do not wish to self-manage the user details anymore) and want 1Kosmos to store their legacy users. There will be a different instance of this for each client. Currently, 1Kosmos is hosting `12` instances.
4. **MongoDB Atlas**: It is a mongo instance that creates a project for each customer to provide LDAP or AD endpoint to manage the LDAP services. There will be a different instance of this for each client.

## BlockID Platform
This structure is the one shown within the red box and is also known as a BlockID Admin Console. There is a main Kubernetes cluster that will be dedicated to the 1kosmos which is also called the main instance and every single customer will get a separate Kubernetes cluster which will be called a client instance. Each customer gets their Kubernetes instance, and a Kubernetes instance will have one tomcat and may have multiple IPFS and Ethereum namespaces depending on the number of tenants. The root instance i.e., 1Kosmos cluster is owned by 1Kosmos and there is only one tenant in it which is 1Kosmos. It consists of the following components:
1. **1Kosmos Cluster main instance (on the left side)**: This Kubernetes cluster is entirely under the control of 1Kosmos. In this cluster, we have three namespaces - tomcat (blockid-console), ipfs, and ethereum components. The namespace is used to organize components and creates a logical boundary around the separate structure of each component. A namespace provides a virtual cluster or space within which you can run applications that are isolated from other applications from other namespaces. Everything that is shared stays inside the namespace and the pods can communicate with each other internally without going outside the cluster. There is a service to service connection that can be made between the pods, internal load balancers, and various other advantages you get by running inside the namespace. The tomcat (blockid-console), ipfs, and ethereum namespaces are explained as follows:
    * **Blockid-console**:
        * **Ingress Router**: It is the only router facing outside the cluster and binds to a publicly routable IP. It is the only entry point to any element inside the cluster. It has a DNS entry, for example, dev.onekosmos.net, that points to one of these environments, and each environment has another copy of it. For example, the Pilot environment has another ingress router with each one of the static IP with a DNS entry pointing to it and similarly the dev and test environment. The ingress router configurations contain the DNS name and whenever we add a new DNS for the customers it first connects to the blockid-console service.
        * **blockid-console (Service)**: It is an internal load balancer that includes several backends (pods) and load balances across the number of pods running behind it. It listens to the pods on ports `80` and `443` and load balances pods labelled with a certain label selector. Using this load balancing service, you can scale the pods from three to any number of pods, for example, from `3` to `10` or from `3` to `1`. The pods are nothing but the blockid-console instances.
        * **blockid-console (Deployment)**: This is the deployment that runs the specified container image in its configurations. Here, the container is a blockid-console, it is a tomcat container, and it contains a war file where the dev team adds the code. When building the container, we fetch the code from Git, run and compile it, and run the war file. After running the war file successfully, stick it to the tomcat base image which is managed by the tomcat project. Every time we build a war file, we put it into the tomcat container and publish it to a container registry (for 1Kosmos). When a cluster wants to deploy that tomcat container and we run or create a deployment then the deployment will specify the container name and number of replicas (usually we create three) of it. The service uses its pod selectors to know what to load balance against. These selectors are the pods named with certain labels inside the blockid console deployment. The deployment object manages the actual deployment and makes sure that each pod is labelled with that same label of deployment. When you scale the deployment, the service automatically starts load balancing across any additional pods, it pulls in any more pods and starts sending traffic to them. When a pod comes up, Kubernetes starts sending traffic to it when it passes the defined readiness probe. The readiness probe is used to check when a container is ready to start accepting traffic. For the BlockID platform, the readiness probe is port `80/helmb`. The readiness probe `localhost:8080/helmb` should return a `200 OK` as a result. Another probe is the liveness probe. This is used to identify the time to restart a container and to check if the pod is still alive. When a pod does not respond to the liveness probe then it will kill the Container and restarts it. For example, if a tomcat stops responding on port `8080` or starts showing an error and not the `200 OK` message page as a health page, the Kubernetes will kill the Container and restarts it.
     * **ipfs**: The blockid-console talks to the ipfs and starts the initial configuration. When the tenant is being configured the console will search for its ipfs service. The MongoDB storage stores the information of which tenant goes to which ipfs cluster.
        * **ipfs-cluster (service)**: The service listens to the pods on port `9095` and `8080` and load balances pods labelled with a certain label selector. The `9095` port is for the API where you can perform all the `Create` and `Delete` object operations. The Read object operation can be done on port `8080` which serves out the requested file.
            :::tip Note:
             You can perform a port translation for port `9095` in the ipfs service.
             :::

       * **ipfs-cluster (Stateful Set)**: The Stateful Set concept of Kubernetes provides the pod identity concept and confirms that the pods have started up in a certain order. It requires the things that need to be bootstrapped in a certain way as required by ipfs. In this ipfs-cluster, the first node is a bootstrapped node, and the subsequent nodes find the first node to join the cluster. The Stateful set also gives the concept of a well-known pod endpoint. So, by starting up the three replicas of the ipfs cluster pod in the Stateful set, each one gets the identity as shown in the Figure. Thus, the first one of the ipfs-cluster will be called as `-0`, the second one as `-1`, and the third one as `-2`. In the bootstrap script of `-0`, it knows that the first one is in line, hence, it does not search for another node. Each node of ipfs-cluster checks for the `-0` node to get that as an initial peer. This is how the Stateful set works, the ipfs bootstraps and the cluster starts up. The cluster finds the first node that has the initial secret or key fuel, and other nodes join the cluster to search that peer key from the first node. Inside each of the ipfs-cluster nodes, there are two containers, one is the ipfs-cluster (top left) container, and the other is the ipfs (daemon). Inside each cluster node, there is an ipfs daemon (bottom right) that runs the ipfs server process and is attached to the storage (which stores the files including content) and serves out data. You can run any number of these ipfs daemons on individual nodes. The ipfs-cluster container exposes a proxy port for the ipfs daemon, exposes the certain proxy APIs. This proxy API substitutes back and exposes the limited functionalities of the ipfs API. It performs two things - the first is to store files and retrieve them from port `8080`. It writes the files to its local daemon but is also aware of other ipfs cluster nodes in its cluster and this is part of a bootstrapping process. The ipfs cluster process includes the main ipfs cluster node (0) and two more ipfs cluster nodes, named `ipfs-1` and `ipfs-2`, respectively. The three ipfs cluster containers know about each other but the ipfs processes (daemons) do not know about anything else, not even their ipfs containers. The ipfs daemons know only about the APIs that they need to expose internally on their API port `5001` and the files that they need to add in its local file system. When you perform a GET file process at port `8080`, it will serve the file on that port. When the ipfs cluster containers receive a file to be added to the cluster, they inform their daemon about the files to save it and signal to the other nodes in the cluster to ask them to add the file in their daemons. Thus, when the ipfs cluster container receives a file, they store it on their local Persistent Volume Claim storage. There are the Persistent Volume block storage devices connected outside the namespace to ipfs-cluster containers and ipfs daemons of each node. When any pod comes up, the Persistent Volume Claim makes the claim for the volume of a certain size, and then Kubernetes talks to its storage system and asks for the persistent system volume that matches up to the claimed size of the volume. For the ipfs-cluster containers, each container needs its local file system as when it goes down and comes back up then it knows where it had left out. The storage volume gets the storage size from the local storage class (extreme right in the namespace). The Storage class provides persistent volume and gets created dynamically. The request of creating a volume further goes to the API and the API creates the volume and binds it to the claim and that volume will then be mounted inside the pod.
      * **ethereum**: 
          * **eth-tx (service)**: In ethereum, configure blockid console on ethereum eth-tx service ports `8545`.
        * **Deployments**:
            * **eth-bootnode**: This deployment contains two bootnode pods of a bootnode container that bootstraps the cluster. Any new miner or the transaction (tx) nodes joining the cluster, find the rest of the cluster by going and talking to the eth-bootnote service. The eth-bootnode deployment returns the list of the bootnodes to the miner or tx nodes. This is how the miner and tx nodes bootstrap themselves and join the cluster.
            * **eth-geth-miner**: This node runs on the network type named Proof of Authority also known as Sealer. This sealer seals every transaction by signing, validating, and adding it to the blockchain.
            * **eth-geth-tx**: This node stores the actual blockchain.
This is how the smart contract is created and transactions are submitted to the cluster.

2. **Kubernetes Cluster for each customer - client instance (on the right side)**: It is a replica of the main 1Kosmos Kubernetes instance for each client and each customer will have one tenant for themselves and they can create more tenants for their clients. When a customer is multi-tenant, it shares the tomcat but has separate IPFS and Ethereum for each of its tenants so that they will have separate storage space for each of their tenants. For example, the replica of the 1Kosmos Kubernetes instance will be created for client A and if client A has its own clients B and C, then each of its clients will have their own sets of IPFS and Ethereum. Hence, client B will have one set of IPFS and Ethereum and client C will have another set of IPFS and Ethereum.
